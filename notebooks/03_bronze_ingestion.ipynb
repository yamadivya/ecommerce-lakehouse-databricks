{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1d74b6-93c6-437a-aaf0-83aa98581ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting: olist_orders_dataset.csv → ecommerce.bronze.raw_orders\nDone! 99,441 records loaded into ecommerce.bronze.raw_orders\n\nIngesting: olist_order_items_dataset.csv → ecommerce.bronze.raw_order_items\nDone! 112,650 records loaded into ecommerce.bronze.raw_order_items\n\nIngesting: olist_customers_dataset.csv → ecommerce.bronze.raw_customers\nDone! 99,441 records loaded into ecommerce.bronze.raw_customers\n\nIngesting: olist_products_dataset.csv → ecommerce.bronze.raw_products\nDone! 32,951 records loaded into ecommerce.bronze.raw_products\n\nIngesting: olist_order_payments_dataset.csv → ecommerce.bronze.raw_payments\nDone! 103,886 records loaded into ecommerce.bronze.raw_payments\n\nIngesting: olist_order_reviews_dataset.csv → ecommerce.bronze.raw_reviews\nDone! 99,224 records loaded into ecommerce.bronze.raw_reviews\n\nIngesting: olist_sellers_dataset.csv → ecommerce.bronze.raw_sellers\nDone! 3,095 records loaded into ecommerce.bronze.raw_sellers\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "landing = \"/Volumes/ecommerce/bronze/landing_zone\"\n",
    "\n",
    "def ingest_csv_to_bronze(source_file, target_table):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and writes to Bronze Delta table.\n",
    "    Adds _ingestion_time and _source_file metadata columns.\n",
    "    \"\"\"\n",
    "    print(f\"Ingesting: {source_file} → {target_table}\")\n",
    "    \n",
    "    df = (\n",
    "        spark.read\n",
    "            .option(\"header\", \"true\")        # first row = column names\n",
    "            .option(\"inferSchema\", \"false\")  # keep everything as STRING (Bronze rule)\n",
    "            .option(\"multiLine\", \"true\")     # handle multi-line text fields (reviews)\n",
    "            .option(\"escape\", '\"')           # handle quoted fields\n",
    "            .csv(f\"{landing}/{source_file}\")\n",
    "            .withColumn(\"_ingestion_time\", F.current_timestamp())\n",
    "            .withColumn(\"_source_file\", F.lit(f\"{landing}/{source_file}\"))\n",
    "    )\n",
    "    \n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")   # overwrite for initial load\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .saveAsTable(target_table)\n",
    "    )\n",
    "    \n",
    "    count = spark.table(target_table).count()\n",
    "    print(f\"Done! {count:,} records loaded into {target_table}\\n\")\n",
    "\n",
    "\n",
    "# ── Ingest all 7 files ──────────────────────────────────────────\n",
    "ingest_csv_to_bronze(\n",
    "    \"olist_orders_dataset.csv\",\n",
    "    \"ecommerce.bronze.raw_orders\"\n",
    ")\n",
    "\n",
    "ingest_csv_to_bronze(\n",
    "    \"olist_order_items_dataset.csv\",\n",
    "    \"ecommerce.bronze.raw_order_items\"\n",
    ")\n",
    "\n",
    "ingest_csv_to_bronze(\n",
    "    \"olist_customers_dataset.csv\",\n",
    "    \"ecommerce.bronze.raw_customers\"\n",
    ")\n",
    "\n",
    "ingest_csv_to_bronze(\n",
    "    \"olist_products_dataset.csv\",\n",
    "    \"ecommerce.bronze.raw_products\"\n",
    ")\n",
    "\n",
    "ingest_csv_to_bronze(\n",
    "    \"olist_order_payments_dataset.csv\",\n",
    "    \"ecommerce.bronze.raw_payments\"\n",
    ")\n",
    "\n",
    "ingest_csv_to_bronze(\n",
    "    \"olist_order_reviews_dataset.csv\",\n",
    "    \"ecommerce.bronze.raw_reviews\"\n",
    ")\n",
    "\n",
    "ingest_csv_to_bronze(\n",
    "    \"olist_sellers_dataset.csv\",\n",
    "    \"ecommerce.bronze.raw_sellers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c170abc7-c0fb-4f06-9a56-2752817a355b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\nBRONZE LAYER — FINAL VERIFICATION\n=======================================================\nraw_orders               99,441 records\nraw_order_items         112,650 records\nraw_customers            99,441 records\nraw_products             32,951 records\nraw_payments            103,886 records\nraw_reviews              99,224 records\nraw_sellers               3,095 records\n---------------------------------------------\nTOTAL                   550,688 records\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 55)\n",
    "print(\"BRONZE LAYER — FINAL VERIFICATION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "tables = {\n",
    "    \"raw_orders\":      \"ecommerce.bronze.raw_orders\",\n",
    "    \"raw_order_items\": \"ecommerce.bronze.raw_order_items\",\n",
    "    \"raw_customers\":   \"ecommerce.bronze.raw_customers\",\n",
    "    \"raw_products\":    \"ecommerce.bronze.raw_products\",\n",
    "    \"raw_payments\":    \"ecommerce.bronze.raw_payments\",\n",
    "    \"raw_reviews\":     \"ecommerce.bronze.raw_reviews\",\n",
    "    \"raw_sellers\":     \"ecommerce.bronze.raw_sellers\",\n",
    "}\n",
    "\n",
    "total_records = 0\n",
    "for name, table in tables.items():\n",
    "    count = spark.table(table).count()\n",
    "    total_records += count\n",
    "    print(f\"{name:<20} {count:>10,} records\")\n",
    "\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'TOTAL':<20} {total_records:>10,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c40481-5c6c-4df8-89b1-641c7ac60ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORDER STATUS distribution:\n+------------+-----+\n|order_status|count|\n+------------+-----+\n|   delivered|96478|\n|     shipped| 1107|\n|    canceled|  625|\n| unavailable|  609|\n|    invoiced|  314|\n|  processing|  301|\n|     created|    5|\n|    approved|    2|\n+------------+-----+\n\nNULL values in raw_orders:\n+--------+-----------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+---------------+------------+\n|order_id|customer_id|order_status|order_purchase_timestamp|order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|_ingestion_time|_source_file|\n+--------+-----------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+---------------+------------+\n|       0|          0|           0|                       0|              160|                        1783|                         2965|                            0|              0|           0|\n+--------+-----------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+---------------+------------+\n\nPAYMENT TYPE distribution:\n+------------+-----+\n|payment_type|count|\n+------------+-----+\n| credit_card|76795|\n|      boleto|19784|\n|     voucher| 5775|\n|  debit_card| 1529|\n| not_defined|    3|\n+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# These are the issues Silver layer will fix\n",
    "\n",
    "print(\"ORDER STATUS distribution:\")\n",
    "spark.table(\"ecommerce.bronze.raw_orders\") \\\n",
    "    .groupBy(\"order_status\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "print(\"NULL values in raw_orders:\")\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when\n",
    "\n",
    "df = spark.table(\"ecommerce.bronze.raw_orders\")\n",
    "null_counts = df.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "null_counts.show()\n",
    "\n",
    "print(\"PAYMENT TYPE distribution:\")\n",
    "spark.table(\"ecommerce.bronze.raw_payments\") \\\n",
    "    .groupBy(\"payment_type\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44eea80-6eaa-4f71-b5cd-b2f91c500fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL VALUES IN raw_orders:\n----------------------------------------\nGood order_id                                 0 nulls\nGood customer_id                              0 nulls\nGood order_status                             0 nulls\nGood order_purchase_timestamp                 0 nulls\nWarning order_approved_at                      160 nulls\nWarning order_delivered_carrier_date          1783 nulls\nWarning order_delivered_customer_date         2965 nulls\nGood order_estimated_delivery_date            0 nulls\nGood _ingestion_time                          0 nulls\nGood _source_file                             0 nulls\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, when\n",
    "\n",
    "df = spark.table(\"ecommerce.bronze.raw_orders\")\n",
    "\n",
    "# Display nulls in a readable vertical format\n",
    "null_counts = [(c, df.filter(col(c).isNull()).count()) for c in df.columns]\n",
    "\n",
    "print(\"NULL VALUES IN raw_orders:\")\n",
    "print(\"-\" * 40)\n",
    "for column, nulls in null_counts:\n",
    "    status = \"Warning\" if nulls > 0 else \"Good\"\n",
    "    print(f\"{status} {column:<35} {nulls:>6} nulls\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}